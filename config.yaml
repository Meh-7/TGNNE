# mvte/config.yaml

data:
  source: "pykeen"          # "files" or "pykeen"
  dataset: "WN18RR"       # only used when source == "pykeen" : "FB15k-237", "WN18RR", "YAGO3-10", etc. 

  prepared_dir: "data/WN18RR/"  # directory where prepared data is stored
  
  train_path: "data/train.txt"
  valid_path: "data/valid.txt"
  test_path: "data/test.txt"

model:
  base_scorer: "rotate"    # "transe", "distmult", "complex", "rotate", etc.
  embedding_dim: 500
  tri_hidden_dim: 128
  tet_hidden_dim: 128
  dropout: 0.1
  gamma: 6.0
  fusion_mode: "equal"   # "learned" | "topo_only" | "equal" | "custom"


topology:
  max_triangles_per_entity: null       # or null for no cap
  max_tetras_per_entity: null          # or null for no cap

training:
  seed: 314
  device: "cuda"                     # "auto", "cpu", "cuda", or "cuda:0"
  lr: 0.001
  num_epochs: 300
  batch_size: 512
  num_negatives: 1024
  adversarial_temperature: 0.5 # alpha for adv. neg. sampling
  negative_mode: "both"              # "head" | "tail" | "both"
  log_interval: 10
  checkpoint_every: 50                # save a checkpoint every N epochs
  #resume_from: checkpoints/FB15k-237/20260130_173506/checkpoint_epoch_200.pt # comment out to train from scratch



evaluation:
  batch_size_entities: 2048
  filtered: true  # whether to use filtered metrics, might be better to set it to false during training to monitor learning curves and true for final evaluation
  hits_ks: [1, 3, 10]
  eval_every: 20                      # run evaluation every N epochs
  max_eval_triples: 15000               # max number of triples to use for eval (for speed during training/validation)

#testing:
#  checkpoint_path: checkpoints/FB15k-237/20251202_141522/checkpoints/best_model.pt

logging:
  level: "INFO"
  log_dir: "logs"

output:
  save_dir: "checkpoints"
